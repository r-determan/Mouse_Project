---
title: "Advanced_models"
output: html_document
date: '2022-04-20'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyr, dplyr, keras)
source("mouse409.R")
```

*Based on page 455... in ISLR Book*

```{r, eval = FALSE}
library (ISLR2)
xdata <- data.matrix (
NYSE[, c("DJ_ return ", " log _ volume "," log _ volatility ")]
)
istrain <- NYSE[, " train "]
xdata <- scale (xdata)


lagm <- function (x, k = 1) {
 n <- nrow (x)
 pad <- matrix (NA , k, ncol (x))
 rbind (pad , x[1:(n - k), ])
}

arframe <- data . frame (log_volume = xdata[, " log _ volume "] ,
L1 = lagm (xdata , 1), L2 = lagm (xdata , 2),
L3 = lagm (xdata , 3), L4 = lagm (xdata , 4),
L5 = lagm (xdata , 5)
)


arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]

arfit <- lm(log_volume ∼ ., data = arframe[istrain , ])
arpred <- predict (arfit , arframe[!istrain , ])
V0 <- var (arframe[!istrain , " log _ volume "])
1- mean ((arpred - arframe[!istrain , " log _ volume "])^2) / V0


arframed <-
data . frame (day = NYSE[-(1:5), " day _of_ week "], arframe)
arfitd <- lm(log_volume ∼ ., data = arframed[istrain , ])
arpredd <- predict (arfitd , arframed[!istrain , ])
1- mean ((arpredd - arframe[!istrain , "log _ volume "])^2) / V0
#------------------------------------------------------------------------
n <- nrow (arframe)
xrnn <- data.matrix (arframe[, -1])
xrnn <- array (xrnn , c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- ap
dim(xrnn)


model <- keras _ model _ sequential () %>%
  layer _ simple _ rnn (units = 12,
                        input_shape = list (5, 3),
                    dropout = 0.1, recurrent_dropout = 0.1) %>%
  layer _ dense (units = 1)
model %>% compile (optimizer = optimizer _ rmsprop (), loss = " mse ")


history <- model %>% fit (
  xrnn[istrain ,, ], arframe[istrain , "log _ volume "],
  batch_size = 64, epochs = 200,
  validation_data =
    list (xrnn[!istrain ,, ], arframe[!istrain , " log _ volume "])
  )
kpred <- predict (model, xrnn[!istrain ,, ])
1- mean ((kpred - arframe[!istrain , " log _ volume "])^2) / V0


model <- keras _ model _ sequential () %>%
layer _ flatten (input_shape = c(5, 3)) %>%
layer _ dense (units = 1)
```

```{r}
n_neurons <- 3
n_lags <- 5


#data_mat <- data.matrix(data_clean %>% select(-closed))
xdata <- data.matrix(data_clean %>% select(-c(open, closed)) %>% select(1:3))
ydata <- data.matrix(data_clean %>% select(open))


#create lag function
lagm <- function (x, k = 1) {
  n <- nrow (x)
  pad <- matrix (NA , k, ncol (x))
  rbind (pad , x[1:(n - k), ])
 }

arframe <- data.frame ( open = ydata ,
L1 = lagm ( xdata , 1) , L2 = lagm ( xdata , 2),
L3 = lagm ( xdata , 3) , L4 = lagm ( xdata , 4) ,
L5 = lagm ( xdata , 5)
)

istrain <- sample(c(TRUE, FALSE), size = nrow(arframe), replace = TRUE, prob = c(.7,.3))

arframe <- arframe [ -(1:n_lags) , ]
istrain <- istrain [ -(1:n_lags) ]
ydata <- ydata[-(1:n_lags)]


n <- nrow ( arframe )
xrnn <- data.matrix ( arframe [ , -1])
xrnn <- array ( xrnn , c (n , n_neurons , n_lags) )
xrnn <- xrnn [ , , n_lags:1]
xrnn <- aperm ( xrnn , c (1 , n_neurons , 2) )
dim ( xrnn )



model <- keras_model_sequential () %>%
  layer_simple_rnn ( units = 12 , 
                     input_shape = list (n_lags , n_neurons) ,
                     dropout = 0.1 , recurrent_dropout = 0.1) %>%
  layer_dense ( units = 1, activation = "sigmoid")
model %>% compile ( optimizer = optimizer_rmsprop() ,
                    loss = "binary_crossentropy" )


history <- model %>% 
  fit(xrnn[ istrain , , ] , arframe[ istrain , "open" ] ,
       batch_size = 64 , epochs = 20 ,
       validation_data = list ( xrnn [! istrain , , ] , arframe [! istrain , "open" ])
)
kpred <- predict ( model , xrnn [! istrain , , ])

y_true <- factor(matrix(ydata[!istrain]))
kpred <- factor(round(kpred,0))

library(caret)
confusionMatrix(kpred, y_true)

names(y_true)


```






```{r}
#arframe <- arframe[-(1:5), ]
#istrain <- istrain[-(1:5)]
x <- model.matrix (log_volume ∼ . - 1, data = arframe)

arnnd <- 
  keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 1, activation = "sigmoid")

arnnd %>% compile(loss = "binary_crossentropy",
                  metrics = c('accuracy'),
                  optimizer = "adam")

history <- arnnd %>%
  fit(x[istrain , ], arframe[istrain , "log_volume"], 
      epochs = 20,
      batch_size = 32, 
      validation_data =list (x[!istrain , ], arframe[!istrain , "log_volume"])
)
plot(history)
npred <- predict (arnnd , x[!istrain , ])
1- mean ((arframe[!istrain , "log_volume"] - npred)^2) / V0


```

