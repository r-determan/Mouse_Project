---
title: "Final Report"
author: "Rose Determan, Shuting Li, Ranfei Xu"
date: 'May 12, 2022'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
pacman::p_load(R.matlab, dplyr,tidyr, caret, factoextra, FactoMineR, pROC,stringr,cvms, keras,kerasR,permute)
```


```{r data prep}
# DATA SETUP
# Import mouse 409 data
bb <- readMat("Data/Zero_Maze/608034_409/Day_1/Trial_001_0/binned_behavior.mat")
bb <- data.frame(t(bb$binned.behavior))
names(bb) <- c("open", "closed")
bz <- readMat("Data/Zero_Maze/608034_409/Day_1/Trial_001_0/binned_zscore.mat")
bz <- data.frame(bz$binned.zscore)

#combine zscore and behavior
data <- data.frame(cbind(bb, bz))
  
#remove rows where no location is coded
data_clean <- data[-which(data$open==0 & data$closed==0),]
data_clean <- data_clean %>% select(-c(closed))
#select only "open" column as indicator
mdl_data <- data_clean 

#remove extra variables
rm(bb, bz, data)


# Import all mice data into one data frame
# read in our data
prefix <- "Data/Zero_Maze/"
folders <- list.files(prefix)

bz_all <- NULL
for (f in folders){
 #abbreviation for file name
 f_abbr <- str_sub(f, -3, -1)
 #subfolder
 sub_f <- list.files(paste0(prefix, f, "/Day_1/Trial_001_0"))
 
 # # MAKE EACH MOUSE IN ITS OWN VARIABLE
 # #binned behavior
 # assign(paste0("bb_",f_abbr), 
 #         readMat(paste0(prefix, f, "/Day_1/Trial_001_0/binned_behavior.mat")))
 # 
 # #z score
 # assign(paste0("bz_",f_abbr), 
 #         readMat(paste0(prefix, f, "/Day_1/Trial_001_0/binned_zscore.mat")))
  
 
 # ADD ALL MICE TO SAME VARIABLE
 #import data
  bb <- readMat(paste0(prefix, f, "/Day_1/Trial_001_0/binned_behavior.mat"))
  bz <- readMat(paste0(prefix, f, "/Day_1/Trial_001_0/binned_zscore.mat"))
 
 #extract dataframes from list
  bb <- t(bb$binned.behavior)
  bz <- data.frame(bz$binned.zscore)
 
 #add behavior to zscore dataframe
  bz$open <- bb[,2]
  bz$closed <- bb[,1]
 #add row number as time proxy
  bz$time <- seq(1:nrow(bz))

  #pivot long and add all data to a single dataframe
  bz_long <- bz %>% pivot_longer(cols = -c(open, closed, time)) %>% mutate(df = f_abbr)
  bz_all <- rbind(bz_all, bz_long)
}

#remove extra variables
rm(bz_long, f, f_abbr,folders, prefix, sub_f, bb, bz)

#select rows were location is coded and select only open column
bz_all <- bz_all[-which(bz_all$open==0 & bz_all$closed==0),]
bz_all <- bz_all %>% select(-closed)


all_results <- data.frame(model_name = NULL,
                          zero_rule_acc = NULL,
                          model_acc = NULL)
```

```{r functions}
plot_cm <- function(result, title = ""){
  cm <- confusion_matrix(targets =result$true, predictions = result$pred)
  plot_confusion_matrix(cm$`Confusion Matrix`[[1]],
                        add_sums = TRUE,
                        add_col_percentages = FALSE,
                        add_row_percentages = FALSE,
                        sums_settings = sum_tile_settings(
              palette = "Oranges",
              label = "Total",
              tc_tile_border_color = "black"
    )) + labs(title = title)
}

accuracy_table <- function(result, train_y, title = "",  all_results){
  # select whichever class is larger in the training data set
  # for testing, always predict the largest class from training set

  n_0 <- sum(train_y == 0)
  n_1 <- sum(train_y== 1)
  zero_rule_acc <- ifelse(n_0>n_1, 
                        sum(result$true == 0)/nrow(result),  
                        sum(result$true == 1)/nrow(result))
  all_results <- rbind(all_results, data.frame(model_name = title,
                                             zero_rule_acc = zero_rule_acc,
                                             model_acc = sum(result$true == result$pred)/nrow(result)))
  return(all_results)
}

n_lags <- 5
#create lag function
lagm <- function (x, k = 1) {
    n <- nrow (x)
    pad <- matrix (NA , k, ncol (x))
    rbind (pad , x[1:(n - k), ])
}

data_prep_lag <- function(data_clean, n_neurons, random_split = 0){
  #n_neurons <- ncol(data_clean)-1
  n_lags <- 5
  
  #data_mat <- data.matrix(data_clean %>% select(-closed))
  xdata <- data.matrix(data_clean %>% select(-c(open)) %>% select(seq(1,n_neurons)))
  ydata <- data.matrix(data_clean %>% select(open))

  #make lags
  arframe <- data.frame ( open = ydata ,
  L1 = lagm ( xdata , 1) , L2 = lagm ( xdata , 2),
  L3 = lagm ( xdata , 3) , L4 = lagm ( xdata , 4) ,
  L5 = lagm ( xdata , 5)
  )
  if (random_split == 0){
  #separate train and test
  istrain <- rep(TRUE, round(nrow(arframe)*0.7/2))
  istrain <- c(istrain, rep(FALSE,nrow(arframe)*0.3/2))
  istrain <- c(istrain,istrain)
  }
  else{
    istrain <- sample(c(TRUE,FALSE),size = nrow(arframe), prob = c(0.7,0.3), replace = TRUE)
  }
  
  #remove na rows due to lags
  arframe <- arframe [ -(1:n_lags) , ]
  istrain <- istrain [ -(1:n_lags) ]
  ydata <- ydata[-(1:n_lags)]
  n <- nrow ( arframe )
  #select only neuron data, including lags -- exclude "open"
  xrnn <- data.matrix ( arframe [ , -1])
  
  #dim(xrnn)
  xrnn <- array ( xrnn , c (n , n_neurons , n_lags) )  #format to n rows; number of neurons columns; number of lags layers
  #dim(xrnn)
  
  xrnn <- xrnn [ , , n_lags:1] #reorder columns 
  #aperm = Transpose an array by permuting its dimensions and optionally resizing it.
  #he final step rearranges the coordinates of the array (like a partial transpose) into the format that the RNN module in keras expects
  xrnn <- aperm ( xrnn , c (1 , 3 , 2) )
  #dim ( xrnn )
  
  out <- list(xrnn, arframe, istrain)
}

mcc <- function(y_true, y_pred){
 TP <- sum((y_true == 1)&(y_pred == 1))
 FP <- sum((y_true == 0)&(y_pred == 1))
 TN <- sum((y_true == 0)&(y_pred == 0))
 FN <- sum((y_true == 1)&(y_pred == 0))
 MCC <- ((TP*TN)-(FP*FN))/sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
return(1 - MCC)
}
```

# Introduction

-- Training/Testing
-- Zero rule accuracy


# Simple Model (Logistic Regression)


## Starting with Mouse 409

```{r}
n_pc <- 25
mdl_data.pca <- PCA(mdl_data[,-1], scale.unit = TRUE, ncp=n_pc, graph = FALSE) #set graph=TRUE can see the arrow plot
eigenvalue <- get_eigenvalue(mdl_data.pca) #80:95.6%; 60:90%; 40: 80.8%


# extract principal components
comp <- data.frame(mdl_data.pca$ind$coord)

# fit model
comp$open <- mdl_data$open

train <- rep(TRUE, round(nrow(comp)*0.7/2))
train <- c(train, rep(FALSE,nrow(comp)*0.3/2))
train <- c(train,train)
training <- comp[train,]
testing <- comp[!train,]

mdl_logis <- glm(open~., data = training, family = binomial("logit"))
#summary(mdl_logis)

pred_logis <- predict(mdl_logis, newdata = testing, type = "response")
error_rate <- mean((pred_logis>.5 & testing$open == 0) | (pred_logis<.5 & testing$open == 1))
#error_rate

pred_logis <- ifelse(pred_logis>=0.5, 1, 0)


result <- data.frame(true = factor(testing$open),
                     pred = factor(pred_logis))
all_results <- accuracy_table(result, training$open,
                              title = "Logistic Reg w/ PCA (first 25 PCs)", 
                              all_results = all_results)
```

```{r fig.cap = "Logistic regression confusion matrix for mouse 409. The accuracy of this model is 0.72 compared to the baseline accuracy of 0.75"}
plot_cm(result = result, title = "Logistic Reg w/ PCA (first 25 PCs)")
rm(comp, eigenvalue, mdl_data.pca, mdl_logis, training, testing, error_rate, pred_logis, train)
```


## All Mice

```{r}
results <- data.frame(pred = NULL, true = NULL, mouse_id = NULL)
acc <- data.frame(mouse_id = NULL, prop_open = NULL, prop_closed = NULL, larger_class = NULL, accuracy = NULL)


for (id in unique(bz_all$df)){
  mdl_data <- bz_all %>% filter(df == id) %>% pivot_wider() %>% select(-c(df,time))
  #print(nrow(mdl_data))
  mdl_data.pca <- PCA(mdl_data[,-1], scale.unit = TRUE, ncp=n_pc, graph = FALSE) 
  eigenvalue <- get_eigenvalue(mdl_data.pca)
  
  # extract principal components
  comp <- data.frame(mdl_data.pca$ind$coord)
  
  # fit model
  comp$open <- mdl_data$open
  
  train <- rep(TRUE, round(nrow(comp)*0.7/2))
  train <- c(train, rep(FALSE,nrow(comp)*0.3/2))
  train <- c(train,train)
  training <- comp[train,]
  testing <- comp[!train,]

  mdl_logis <- glm(open~., data = training, family = binomial("logit"))
  pred_logis <- predict(mdl_logis, newdata = testing, type = "response")
  error_rate <- mean((pred_logis>.5 & testing$open == 0) | (pred_logis<.5 & testing$open == 1))
  #print(error_rate)

  logisROC <- roc(testing$open, pred_logis)
  assign(paste0("roc_",id),logisROC) 
  #---
  pred_logis <- ifelse(pred_logis>=0.5, 1, 0)
  
  results <- rbind(results, data.frame(pred = factor(pred_logis),true = factor(testing$open), mouse_id = id))
  acc <- rbind(acc, data.frame(mouse_id = id, 
                               prop_open = sum(mdl_data$open)/ nrow(mdl_data),
                               prop_closed = (nrow(mdl_data)-sum(mdl_data$open))/ nrow(mdl_data),
                               accuracy = 1-error_rate))


}

acc <- acc%>%  rowwise %>%mutate(larger_class =  max(prop_open, prop_closed))
```

```{r fig.cap="Confusion matrix for all mice models. "}
# Across ALL models
cm <- confusion_matrix(predictions = results$pred, targets = results$true)

plot_confusion_matrix(cm$`Confusion Matrix`[[1]],
                      add_sums = TRUE,
                      add_col_percentages = FALSE,
                      add_row_percentages = FALSE,
                      sums_settings = sum_tile_settings(
            palette = "Oranges",
            label = "Total",
            tc_tile_border_color = "black"
  )) + labs(title = "Total for All 11 Mice/Models")

```

```{r fig.cap = "ROC for each mouse logistic regression model"}
ggroc(list("251" = roc_251, "254" = roc_254,
           "255" = roc_255, "256" = roc_256,
           "274" = roc_274, "409" = roc_409,
           "412" = roc_412, "414" = roc_414,
           "416" = roc_416,
           "417" = roc_417, "418" = roc_418))


ggplot(data = acc,aes(x = mouse_id))+
  geom_errorbar(aes(ymax = accuracy, ymin = larger_class, x = mouse_id,y = accuracy),width = 0)+
  geom_point(aes(y = accuracy, color = "Model Accuracy"), size = 3)+
  geom_point(aes(y = larger_class, color = "Zero Rule Accuracy"), size = 3)+
  labs(x = "Mouse ID", y = "Accuracy", title = "Accuracy of Test Data for Each Mouse/Model", subtitle= "Where \"Zero Rule\" is always selecting the larger class \nGoal = Model Acc > Zero Rule Acc", color = "Accuracy")


rm(cm, comp, eigenvalue, logisROC, mdl_data, mdl_data.pca, mdl_logis, results,
   roc_251, roc_254, roc_255, roc_256,roc_257, roc_258, roc_274, roc_409, roc_412, roc_414, roc_416, roc_417, roc_418, 
   testing, training, error_rate, id, n_pc, plt, pred_logis, train, acc)
```






# Neural Network Models

## RNN with time lags 

**Reason we try this model: Does the behavior impact neural activity? We assume the past location has influence on the current state of the neurons. **

```{r}
##mouse 409
#data_clean <- data_clean %>% mutate(open = as.numeric(open))

for (i in c(1,3,5)){
  behav <- c(rep(NA,i), data_clean$open[1:(length(data_clean$open)-i)])
  data_clean_behav <- cbind(behav, data_clean %>% select(-c(open))) %>% na.omit()
  istrain <- rep(TRUE, round(nrow(data_clean_behav)*0.7/2))
  istrain <- c(istrain, rep(FALSE,nrow(data_clean_behav)*0.3/2))
  istrain <- c(istrain,istrain)
    
  train <- data_clean_behav[istrain,]
  X_train <- train %>% select(-c(behav))
  X_train <- as.matrix(X_train)
  
  y_train <- train %>% select(behav)
  y_train <- as.matrix(y_train)
  
  test <-  data_clean_behav[!istrain,]
  X_test <- test %>% select(-c(behav))
  X_test <- as.matrix(X_test)
  y_test <- test %>% select(behav)
  y_test <- as.matrix(y_test)
  
  model <- keras_model_sequential() 
  
  # Add layers to the model
  model %>% 
      layer_dense(units = 100, activation = 'relu', input_shape = c(ncol(X_train))) %>% 
      layer_dense(units = 50, activation = 'relu') %>% 
      layer_dense(units = 1, activation = 'sigmoid')
  
  
  history <- model %>% 
    compile(
     loss = "binary_crossentropy",
     optimizer = optimizer_rmsprop(),
     metrics = c("accuracy"),
  )%>% 
    fit(X_train, y_train,
                epochs = 150, 
                batch_size = 50,
                validation_split = 0.3, verbose = 0)
  
  pred <- model %>% predict(X_test)
  pred <- ifelse(pred>=0.5, 1, 0)
  
  result <- data.frame(true = factor(y_test), pred = factor(pred))
  
  all_results <- accuracy_table(result,train_y = y_train,
                                title = paste0("Neural Net with ", i, " Level Shift of Outcome"), 
                                all_results = all_results)
}

rm(result, zero_rule_acc, model, pred, train, test, history, X_test, X_train, y_test,y_train, istrain)

all_results
```

 
## LSTM and Bidirectional LSTM

```{r}
### here, we've commented out the model fits, and saved the results in a csv, since
### the models can take some time to fit. This way the document knits a bit faster


# epochs = 150
# n_lags <- 5
# all_mice_mdl<- NULL
# 
# 
# for (mouse in unique(bz_all$df)){
#   result <- NULL
#   tStart <- Sys.time()
#   print(paste(mouse, "start"))
#   data <- bz_all %>% filter(df == mouse) %>% select(-c(df)) %>% pivot_wider() %>% select(-time)
#   n_neurons <- ncol(data)-1
# 
# 
#   out <- data_prep_lag(data ,n_neurons )
#   xrnn <- out[[1]]
#   arframe <-  out[[2]]
#   istrain <- out[[3]]
#   ydata <- arframe$open
# 
#   # BIDIRECTIONAL LSTM
#   model <- keras_model_sequential () %>%
#     bidirectional(layer_lstm(units = 50, input_shape = list(n_lags , n_neurons))) %>%
#     layer_dense ( units = 1, activation = "sigmoid")
#   model %>% compile ( optimizer = optimizer_rmsprop() ,
#                       loss = "binary_crossentropy",
#                       metrics = c("accuracy") )
#   history <- model %>%
#     fit(xrnn[ istrain , , ] , arframe[ istrain , "open" ] ,
#         batch_size = 50 , epochs = epochs ,
#         validation_data = list ( xrnn [! istrain , , ] , arframe [! istrain , "open" ]),
#         verbose = 0
#     )
#   kpred <- predict ( model , xrnn [! istrain , , ])
#   y_true <- factor(matrix(ydata[!istrain]))
#   kpred <- factor(round(kpred,0))
# 
# 
#   result <- data.frame(true = y_true, pred = kpred)
#   result$true <- factor(result$true, levels = c(0,1))
#   result$pred <- factor(result$pred, levels = c(0,1))
# 
#   all_mice_mdl <- accuracy_table(result, train_y = ydata[istrain],paste0("BILSTM_",mouse), all_mice_mdl)
# 
#   # REGULAR LSTM
#   model <- keras_model_sequential () %>%
#     layer_lstm(units = 50, input_shape = list(n_lags , n_neurons)) %>%
#     layer_dense ( units = 1, activation = "sigmoid")
#   model %>% compile ( optimizer = optimizer_rmsprop() ,
#                       loss = "binary_crossentropy",
#                       metrics = c("accuracy") )
#   history <- model %>%
#     fit(xrnn[ istrain , , ] , arframe[ istrain , "open" ] ,
#         batch_size = 50 , epochs = epochs ,
#         validation_data = list ( xrnn [! istrain , , ] , arframe [! istrain , "open" ]),
#         verbose = 0
#     )
#   kpred <- predict ( model , xrnn [! istrain , , ])
#   y_true <- factor(matrix(ydata[!istrain]))
#   kpred <- factor(round(kpred,0))
# 
# 
#   result <- data.frame(true = y_true, pred = kpred)
#   result$true <- factor(result$true, levels = c(0,1))
#   result$pred <- factor(result$pred, levels = c(0,1))
# 
#   all_mice_mdl <- accuracy_table(result, train_y = ydata[istrain],paste0("LSTM_",mouse), all_mice_mdl)
# 
# 
# 
#   print(paste(mouse, "end.", Sys.time() - tStart))
# 
# 
# }
# res <- all_mice_mdl
# res <- res %>% mutate(mouse = str_extract(model_name, "\\d{3}"),
#                model = str_extract(model_name, "\\w{4,6}(?=_)")) %>% 
#   select(-c(model_name)) %>% 
#   pivot_wider(names_from = model, values_from = model_acc)
# 
# write.csv(res, "final_model_results.csv")


res <- read.csv("final_model_results.csv")
res$mouse <- as.factor(res$mouse)
```

```{r}
ggplot(data = res, aes(x =mouse))+
  geom_point(mapping = aes(y = zero_rule_acc, color = "Baseline"), size = 3)+
  geom_point(mapping = aes(y = BILSTM, color = "BI-LSTM"), size = 3)+
  geom_point(mapping = aes(y = LSTM, color = "LSTM"), size = 3)


ggplot(data = all_mice_mdl,aes(x = model_name))+
  geom_errorbar(aes(ymax = model_acc, ymin = zero_rule_acc, x = model_name,y = model_acc),width = 0)+
  geom_point(aes(y = model_acc, color = "Model Accuracy"), size = 3)+
  geom_point(aes(y = zero_rule_acc, color = "Zero Rule Accuracy"), size = 3)+
  labs(x = "Mouse ID", y = "Accuracy", title = "Accuracy of Test Data for Each Mouse/Model", subtitle= "One Bidirectional LSTM Model fit for each mouse w/ all available neurons", color = "Accuracy")
```


# Conclusions

# Appendix


## Incorrect Version w/ randomly selected train/test : Mouse 409
**At first, we incorrectly split the training and testing set randomly. The below shows the** ***incorrect*** **model. Interestingly, this incorrectly prepared model performs better than the RNN that is correctly setup in the next section **

```{r}
n_lags <- 5
#create lag function
lagm <- function (x, k = 1) {
    n <- nrow (x)
    pad <- matrix (NA , k, ncol (x))
    rbind (pad , x[1:(n - k), ])
}

n_neurons <- 25
epochs = 150
out <- data_prep_lag(data_clean ,n_neurons , random_split = 1)
xrnn <- out[[1]]
arframe <-  out[[2]]
istrain <- out[[3]]
ydata <- arframe$open

model <- keras_model_sequential () %>%
  layer_simple_rnn ( units = 100 , 
                     input_shape = list (n_lags , n_neurons) ,
                     activation = "relu") %>%
  layer_dense ( units = 50, activation = "relu") %>% 
  layer_dense ( units = 1, activation = "sigmoid")
model %>% compile ( optimizer = optimizer_rmsprop() ,
                    loss = "binary_crossentropy",
                    metrics = c("accuracy") )
history <- model %>% 
  fit(xrnn[ istrain , , ] , arframe[ istrain , "open" ] ,
       batch_size = 50 , epochs = epochs ,
       validation_data = list ( xrnn [! istrain , , ] , arframe [! istrain , "open" ]), 
      verbose = 0
)
kpred <- predict ( model , xrnn [! istrain , , ])
y_true <- factor(matrix(ydata[!istrain]))
kpred <- factor(round(kpred,0))

summary(model)

plot(history)

result <- data.frame(true =y_true, pred = kpred)

plot_cm(result = result, title = "Incorrect RNN w/ 25 Neurons")
all_results <- accuracy_table(result,train_y = ydata[istrain],
                              title = "Incorrect RNN w/ 25 Neurons", 
                              all_results = all_results)

rm(arframe, xdata, epochs, istrain, model, n, n_neurons, xrnn, y_true, ydata, zero_rule_acc, cm, history, result, kpred)
```





