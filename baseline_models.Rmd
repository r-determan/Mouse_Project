---
title: "Models"
output: html_notebook
---
```{r setup}
pacman::p_load(R.matlab, dplyr, caret, factoextra, FactoMineR, pROC)
```

# Data Setup/ Import
```{r}
# Import mouse 409 data
bb <- readMat("Data/Zero_Maze/608034_409/Day_1/Trial_001_0/binned_behavior.mat")
bb <- data.frame(t(bb$binned.behavior))
names(bb) <- c("open", "closed")

bz <- readMat("Data/Zero_Maze/608034_409/Day_1/Trial_001_0/binned_zscore.mat")
bz <- data.frame(bz$binned.zscore)

data <- data.frame(cbind(bb, bz))
data$time <- seq(1:nrow(data))
  
data_clean <- data[-which(data$open==0 & data$closed==0),]

rm(bb, bz, data)
```

## Logistic Model with Random Neurons
```{r}

mdl_data <- data_clean %>% select(-closed)

a <- NULL
for (i in 1:100){
  for (s in c(1,5,25,50,100)){
  sample <- sample(2:ncol(mdl_data), size = s)
  train <- sample(c(TRUE, FALSE), size = nrow(mdl_data), replace = TRUE)
  train_data <- mdl_data[train, c(1,sample)]
  test_data <- mdl_data[!train, c(1,sample)]
  mdl <- glm(open~., data = train_data, family = binomial(link = "logit"))
  pred <- predict(mdl, test_data, type = "response") 
  pred <- factor(ifelse(pred>=0.5, 1, 0), levels = c(1,0))
  cm <- confusionMatrix(data = pred, reference = factor(test_data$open, levels = c(1,0)))
  a <- rbind(a,c(cm$overall["Accuracy"], s) )
  }
  #print(i)
}
a <- data.frame(a)

ggplot(data = a)+
  geom_histogram(mapping = aes(x = Accuracy))+
  facet_wrap(vars(V2))

means <- a %>% group_by(V2) %>% summarise(mean = mean(Accuracy)) 
means[which.max(means$mean),]
```

## Logistic Model Combined With PCA (components = 40)
```{r}
mdl_data.pca <- PCA(mdl_data[,-1], scale.unit = TRUE, ncp=40, graph = FALSE) #set graph=TRUE can see the arrow plot
eigenvalue <- get_eigenvalue(mdl_data.pca) #80:95.6%; 60:90%; 40: 80.8%

# extract principal components
comp <- data.frame(mdl_data.pca$ind$coord)

# fit model
comp$open <- mdl_data$open
set.seed(1)
train <- sample(1:nrow(comp), size = round(.8*nrow(comp)), replace = FALSE)
training <- comp[train,]
testing <- comp[-train,]
mdl_logis <- glm(open~., data = training, family = binomial("logit"))
summary(mdl_logis)
pred_logis <- predict(mdl_logis, newdata = testing)
error_rate <- mean((pred_logis>.5 & testing$open == 0) | (pred_logis<.5 & testing$open == 1))
error_rate
```

## ROC Curve
```{r}
logisROC <- roc(testing$open, pred_logis)
plot(logisROC, print.auc=TRUE, auc.polygen=TRUE)
```

### Construct a Function to Compare Different Number of Components
```{r}
error_rate <- NULL
for (i in 1:(ncol(mdl_data)-1)){
  mdl_data.pca <- PCA(mdl_data[,-1], scale.unit = TRUE, ncp=i, graph = FALSE)
  comp <- data.frame(mdl_data.pca$ind$coord)
  comp$open <- mdl_data$open
  set.seed(1)
  train <- sample(1:nrow(comp), size = round(.8*nrow(comp)), replace = FALSE)
  training <- comp[train,]
  testing <- comp[-train,]
  mdl_logis <- glm(open~., data = training, family = binomial("logit"))
  pred_logis <- predict(mdl_logis, newdata = testing)
  error_rate[i] <- mean((pred_logis>.5 & testing$open == 0) | (pred_logis<.5 & testing$open == 1))
}
plot(x=1:111, y=error_rate, type = "l", xlab="Number of Principal Components", col="blue")
```

