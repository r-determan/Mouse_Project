--
title: "Advanced_models"
output: html_document
date: '2022-04-20'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyr, dplyr, keras, caret)
source("mouse409.R")
```






*Based on page 455... in ISLR Book*

```{r}
n_neurons <- 
n_lags <- 5
epochs <- 50

#data_mat <- data.matrix(data_clean %>% select(-closed))
xdata <- data.matrix(data_clean %>% select(-c(open, closed)) %>% select(1:25))
ydata <- data.matrix(data_clean %>% select(open))
#create lag function
lagm <- function (x, k = 1) {
  n <- nrow (x)
  pad <- matrix (NA , k, ncol (x))
  rbind (pad , x[1:(n - k), ])
 }
#make lags
arframe <- data.frame ( open = ydata ,
L1 = lagm ( xdata , 1) , L2 = lagm ( xdata , 2),
L3 = lagm ( xdata , 3) , L4 = lagm ( xdata , 4) ,
L5 = lagm ( xdata , 5)
)
#separate train and test
#istrain <- sample(c(TRUE, FALSE), size = nrow(arframe), replace = TRUE, prob = c(.7,.3))


istrain <- rep(TRUE, round(nrow(arframe)*0.7/3))
istrain <- c(istrain, rep(FALSE,nrow(arframe)/3-round(nrow(arframe)*0.7/3)))
istrain <- c(istrain, istrain,istrain, FALSE)


nrow(arframe) ==length(istrain)
```

```{r}

#remove na rows due to lags
arframe <- arframe [ -(1:n_lags) , ]
istrain <- istrain [ -(1:n_lags) ]
ydata <- ydata[-(1:n_lags)]
n <- nrow ( arframe )
#select only neuron data, including lags -- exclude "open"
xrnn <- data.matrix ( arframe [ , -1])

dim(xrnn)
xrnn <- array ( xrnn , c (n , n_neurons , n_lags) )  #format to n rows; number of neurons columns; number of lags layers
dim(xrnn)
xrnn <- xrnn [ , , n_lags:1] #reorder columns 
#aperm = Transpose an array by permuting its dimensions and optionally resizing it.
#he final step rearranges the coordinates of the array (like a partial transpose) into the format that the RNN module in keras expects
xrnn <- aperm ( xrnn , c (1 , 3 , 2) )
dim ( xrnn )

```


```{r}
model <- keras_model_sequential () %>%
  layer_simple_rnn ( units = 100 , 
                     input_shape = list (n_lags , n_neurons) ,
                     activation = "relu") %>%
  layer_dense ( units = 50, activation = "relu") %>% 
  layer_dense ( units = 1, activation = "sigmoid")
model %>% compile ( optimizer = optimizer_rmsprop() ,
                    loss = "binary_crossentropy",
                    metrics = c("accuracy") )
history <- model %>% 
  fit(xrnn[ istrain , , ] , arframe[ istrain , "open" ] ,
       batch_size = 2 , epochs = epochs ,
       validation_data = list ( xrnn [! istrain , , ] , arframe [! istrain , "open" ])
)
kpred <- predict ( model , xrnn [! istrain , , ])
y_true <- factor(matrix(ydata[!istrain]))
kpred <- factor(round(kpred,0))
confusionMatrix(kpred, y_true)



sum(arframe[ istrain , "open" ])/length(arframe[ istrain , "open" ])
sum(arframe[ !istrain , "open" ])/length(arframe[ !istrain , "open" ])
```

# Next steps 
val_accuracy: 0.8512 lag=10 75n, without 2nd relu layer
val_accuracy: 0.8625 lag = 5 75n, without 2nd relu layer
val_accuracy: 0.8340 lag = 5 75n, with 2nd relu layer

- try implementing PCA then use PCs as input for model
- try train,test,train,test
- try random forest similar 